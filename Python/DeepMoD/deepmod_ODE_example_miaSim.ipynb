{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of the DeepMoD Application to the miaSimS4 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow.core.util import event_pb2\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# DeepMoD functions\n",
    "# load the deepymod package from github.com/mariaproebstl/DeePyMoD.git\n",
    "from deepymod import DeepMoD\n",
    "from deepymod.data import Dataset, get_train_test_loader\n",
    "from deepymod.model.func_approx import *\n",
    "from deepymod.model.constraint import LeastSquares\n",
    "from deepymod.model.sparse_estimators import Threshold, PDEFIND\n",
    "from deepymod.training import train\n",
    "from deepymod.training.sparsity_scheduler import TrainTestPeriodic\n",
    "from deepymod.model.libraryODE import LibraryODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(0)\n",
    "\n",
    "# Configuring GPU or CPU\n",
    "if False: # torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = \"miaSimS4_test2\"\n",
    "filename = \"miaSim_GLV_4species_new.csv\"\n",
    "\n",
    "int_order = 2\n",
    "\n",
    "hl_number = 5\n",
    "hl_size = 100\n",
    "threshold = 0.01\n",
    "\n",
    "max_iterations = 500\n",
    "\n",
    "set_threshold = True\n",
    "only_fitting = False\n",
    "\n",
    "# specify how often data is written to tensorboard and checks train loss , by default 25.\n",
    "write_iterations = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input and Output folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify input and output folder\n",
    "input_folder = \"MScThesis/explore/data/final_datasets/\"\n",
    "output_folder = \"deepmod_output/\"\n",
    "\n",
    "# folderpaths for output\n",
    "folderpath_out = f\"{output_folder}output_{data_name}\"\n",
    "folderpath_plots = f'{folderpath_out}/Plots'\n",
    "folderpath_data = f'{folderpath_out}/Data'\n",
    "\n",
    "# create output folder\n",
    "if not os.path.exists(folderpath_out):\n",
    "    os.makedirs(folderpath_out)\n",
    "    os.makedirs(folderpath_plots)\n",
    "    os.makedirs(folderpath_data)\n",
    "    \n",
    "# path of data file (input)\n",
    "filepath = input_folder + filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the parameters are initialized for miaSimS4_test2:\n",
      "\n",
      "        input file: miaSim_GLV_4species_new.csv\n",
      "\n",
      "        hidden layers: number=5, size=100\n",
      "\n",
      "        order of interactions: 2 \n",
      "\n",
      "        max. iterations: 500\n",
      "\n",
      "        set_threshold: True \n",
      "\n",
      "        threshold: 0.01\n",
      "\n",
      "        device = cpu\n",
      "\n",
      "        only_fitting = False\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"the parameters are initialized for {data_name}:\\n\n",
    "        input file: {filename}\\n\n",
    "        hidden layers: number={hl_number}, size={hl_size}\\n\n",
    "        order of interactions: {int_order} \\n\n",
    "        max. iterations: {max_iterations}\\n\n",
    "        set_threshold: {set_threshold} \\n\n",
    "        threshold: {threshold}\\n\n",
    "        device = {device}\\n\n",
    "        only_fitting = {only_fitting}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to import the datafile and put it into the right format\n",
    "def create_data():\n",
    "    data = pd.read_csv(filepath, sep=\",\", header=0)\n",
    "    usol = data.to_numpy()\n",
    "    ts = usol[:, 0]\n",
    "    data_y = usol[:, 1:]\n",
    "\n",
    "    # set dimensions of the dataset\n",
    "    global n_samples, n_taxa\n",
    "    n_samples, n_taxa = data_y.shape\n",
    "\n",
    "    # plot the raw data\n",
    "    fig, ax = plt.subplots()\n",
    "    for i in np.arange(n_taxa):\n",
    "        ax.plot(ts, data_y[:, i], label=f\"x{i+1}\")\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"Abundance\")\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.savefig(f'{folderpath_plots}/plot_dataset.png', \n",
    "                bbox_inches='tight', dpi = 200)\n",
    "    plt.close()\n",
    "\n",
    "    T = torch.from_numpy(ts.reshape(-1, 1)).float()\n",
    "    Y = torch.from_numpy(data_y).float()\n",
    "\n",
    "    # save names of all taxa in csv file\n",
    "    names = list(data.columns)[1:]\n",
    "    x_i = [f\"x{i+1}\" for i in np.arange(n_taxa)]\n",
    "    df_names = pd.DataFrame({'x_i':x_i, 'Names':names})\n",
    "    df_names.to_csv(f\"{folderpath_data}/Names.csv\", index=False)\n",
    "\n",
    "    return T, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = Dataset(\n",
    "    create_data,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring DeepMoD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help function\n",
    "def access_TFRecordDataset(out_var, log_path):\n",
    "\n",
    "    out_var_dir = log_path + \"/\" + out_var + \"/\"\n",
    "    tmp_file = os.listdir(out_var_dir)[-1]\n",
    "    file_dir = out_var_dir + tmp_file\n",
    "\n",
    "    out = np.array([])\n",
    "    index = np.array([])\n",
    "    i = 0\n",
    "    for serialized_example in tf.data.TFRecordDataset(file_dir):\n",
    "        event = event_pb2.Event.FromString(serialized_example.numpy())\n",
    "        for value in event.summary.value:\n",
    "            # Extract relevant information from the event\n",
    "            val = value.simple_value\n",
    "            out = np.append(out, val)\n",
    "            index = np.append(index, (i+1)*write_iterations)\n",
    "            i += 1\n",
    "    \n",
    "    # save values\n",
    "    df_tmp = pd.DataFrame({'Iteration': index, 'Value': out})\n",
    "    df_tmp.to_csv(f\"{log_path}/Data/{out_var}.csv\", index=False)\n",
    "    \n",
    "    return [index, out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed = 1699534442\n"
     ]
    }
   ],
   "source": [
    "# Settings for reproducibility\n",
    "seed_value = int(time.time())\n",
    "np.random.seed(seed_value)\n",
    "print(f\"seed = {seed_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, test_dataloader = get_train_test_loader(\n",
    "    dataset, train_test_split=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_shape=[hl_size, hl_number]\n",
    "hidden_layer = list(np.repeat(network_shape[0], network_shape[1]))\n",
    "\n",
    "network = NN(1, hidden_layer, n_taxa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library $\\Theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library function\n",
    "library = LibraryODE(int_order=int_order, intercept=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration of the sparsity estimator and sparsity scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration of the sparsity estimator and sparsity scheduler\n",
    "estimator = Threshold(threshold)\n",
    "sparsity_scheduler = TrainTestPeriodic(\n",
    "    periodicity=100, patience=200, delta=1e-5)\n",
    "\n",
    "constraint = LeastSquares()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model\n",
    "model = DeepMoD(network, library, estimator, constraint)  # .to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), betas=(0.99, 0.99), amsgrad=True, lr=5e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run DeepMoD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory for train output\n",
    "log_path = f'{folderpath_out}/train_log'\n",
    "if not os.path.exists(log_path):\n",
    "    os.makedirs(log_path)\n",
    "    os.makedirs(f'{log_path}/Plots/')\n",
    "    os.makedirs(f'{log_path}/Data/')\n",
    "\n",
    "# log print output of train()\n",
    "old_stdout = sys.stdout\n",
    "log_file = open(f\"{folderpath_out}/log_iterations.log\", \"w\")\n",
    "sys.stdout = log_file\n",
    "\n",
    "# Settings for reproducibility\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "train(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    test_dataloader,\n",
    "    optimizer,\n",
    "    sparsity_scheduler,\n",
    "    log_dir=log_path,\n",
    "    max_iterations=max_iterations,\n",
    "    sparsity_update=set_threshold,\n",
    "    only_fitting=only_fitting,\n",
    "    delta=1e-5\n",
    ")\n",
    "\n",
    "# close log file again\n",
    "sys.stdout = old_stdout\n",
    "log_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save structure of the library (list of coefficients contained in the library)\n",
    "library_values = model.library.get_content(dataset.data)\n",
    "df_library_values = pd.DataFrame()\n",
    "idx = 0\n",
    "for ls in library_values:\n",
    "    df_tmp = pd.DataFrame(ls)\n",
    "    df_library_values[f\"x{idx+1}\"] = df_tmp\n",
    "    idx += 1\n",
    "\n",
    "df_library_values.to_csv(\n",
    "    f\"{folderpath_data}/model_library_values.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save sparsity mask and estimated coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of coefficients per taxon\n",
    "n_coefs = len(library_values[0])\n",
    "\n",
    "# sparsity masks\n",
    "df_sparsity_masks = pd.DataFrame()\n",
    "idx = 0\n",
    "for ls in model.sparsity_masks:\n",
    "    np_tmp = ls.numpy()\n",
    "    df_tmp = pd.DataFrame(np_tmp)\n",
    "    df_sparsity_masks[f\"x{idx+1}\"] = df_tmp\n",
    "    idx += 1\n",
    "\n",
    "df_sparsity_masks.to_csv(\n",
    "    f\"{folderpath_data}/model_sparsity_masks.csv\")\n",
    "\n",
    "# estimation coefficients\n",
    "df_estimated_coeffs = pd.DataFrame()\n",
    "idx = 0\n",
    "for ls in model.estimator_coeffs():\n",
    "    df_tmp = pd.DataFrame(ls)\n",
    "    df_estimated_coeffs[f\"x{idx+1}\"] = df_tmp\n",
    "    idx += 1\n",
    "# change names of y axis\n",
    "ylabels = library_values[0]\n",
    "df_estimated_coeffs = df_estimated_coeffs.set_axis(ylabels, axis=0)\n",
    "# save table as csv\n",
    "df_estimated_coeffs.to_csv(\n",
    "    f\"{folderpath_data}/model_estimated_coeffs.csv\")\n",
    "    \n",
    "# define labels for heatmap\n",
    "results = np.asarray(df_estimated_coeffs.transpose())\n",
    "strings = np.asarray(df_library_values.transpose())\n",
    "labels = (np.asarray([\"{0}\\n{1:.2f}\".format(string, value)\n",
    "                    for string, value in zip(strings.flatten(),\n",
    "                                            results.flatten())])\n",
    "        ).reshape(n_taxa, df_library_values.shape[0])\n",
    "# make heatmap and save as png\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_figwidth(1.5*df_library_values.shape[0])\n",
    "fig.set_figheight(n_taxa)\n",
    "ax = sns.heatmap(df_estimated_coeffs.transpose(), cmap=\"RdBu\", center= 0, annot=labels, fmt=\"\", xticklabels=False)\n",
    "ax.tick_params(top=False)\n",
    "plt.yticks(rotation=0)\n",
    "plt.savefig(f'{folderpath_plots}/model_estimated_coeffs.png',\n",
    "            bbox_inches='tight', dpi = 200)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis/Visualization of the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for taxon_tmp in np.arange(n_taxa):\n",
    "\n",
    "    loss_mse = access_TFRecordDataset(\n",
    "        f\"loss_mse_output_{taxon_tmp}\", log_path)\n",
    "    loss_reg = access_TFRecordDataset(\n",
    "        f\"loss_reg_output_{taxon_tmp}\", log_path)\n",
    "    MSE_test = access_TFRecordDataset(\n",
    "        f\"remaining_MSE_test_val_{taxon_tmp}\", log_path)\n",
    "    loss_l1 = access_TFRecordDataset(f\"loss_l1_output_{taxon_tmp}\", log_path)\n",
    "\n",
    "    # plot mse and reg loss\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(loss_mse[0], loss_mse[1],\n",
    "            c='#002635', marker='o', label='MSE loss')\n",
    "    ax.plot(loss_reg[0], loss_reg[1],\n",
    "            c='gray', marker='o', ls='--', alpha=0.6, label='Reg loss')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Cost')\n",
    "    ax.set_title(f'loss for x{taxon_tmp+1}')\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.savefig(f'{folderpath_plots}/loss_plot_x{taxon_tmp+1}.png',\n",
    "        bbox_inches='tight', dpi = 200)\n",
    "    plt.close()\n",
    "\n",
    "    # plot and save estimated coefs per iteration\n",
    "    output = []\n",
    "\n",
    "    for coef in np.arange(n_coefs):\n",
    "        output_coef = access_TFRecordDataset(\n",
    "            f\"estimator_coeffs_output_{taxon_tmp}_coeff_{coef}\", log_path)\n",
    "        output.append(output_coef)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    for coef in np.arange(n_coefs):\n",
    "        ax.scatter(output[coef][0], output[coef][1], \n",
    "                    label=f'{library_values[taxon_tmp][coef]}', s=1)\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Coefficient')\n",
    "    ax.set_title(f'Coefficients for x{taxon_tmp+1}')\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.savefig(\n",
    "        f'{folderpath_plots}/estimated_coeffs_x{taxon_tmp+1}.png',\n",
    "        bbox_inches='tight', dpi = 200)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model reached max_iterations (500).\n"
     ]
    }
   ],
   "source": [
    "# check how many iterations were needed for the training of the model\n",
    "last_iteration = int(output[coef][0][-1])\n",
    "if last_iteration==max_iterations:\n",
    "    print(f\"model reached max_iterations ({last_iteration}).\")\n",
    "elif last_iteration < max_iterations:\n",
    "    print(f\"model converged at iteration {last_iteration}.\")\n",
    "else:\n",
    "    print(f\"Error: last iteration is {last_iteration}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch all Plots\n",
    "for file_name in os.listdir(f\"{log_path}/Plots/\"):\n",
    "    # construct full file path\n",
    "    source = f\"{log_path}/Plots/\" + file_name\n",
    "    destination = folderpath_plots + \"/\" + file_name\n",
    "    shutil.move(source, destination)\n",
    "# fetch all data files\n",
    "for file_name in os.listdir(f\"{log_path}/Data/\"):\n",
    "    # construct full file path\n",
    "    source = f\"{log_path}/Data/\" + file_name\n",
    "    destination = folderpath_data + \"/\" + file_name\n",
    "    shutil.move(source, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove log folder with all training output\n",
    "shutil.rmtree(log_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeePyMoD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
